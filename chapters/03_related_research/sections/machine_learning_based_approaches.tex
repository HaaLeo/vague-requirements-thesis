\section{Machine Learning Based Approaches}
\label{chp:related_research:sec:machine_learingn_based_approaches}

Next to solely rule-based approaches, there exist \ac{ML} based approaches which use \ac{ML} techniques to classify a requirement.
In this chapter \ac{ML} based approaches for requirement classification are presented.

\textcite{Medlock:2007} present an approach which examines hedge classification in scientific literature.
The subject of hedge classification is to detect speculative language.
First they introduce a probabilistic model to gather training data.
The same model is used to derive a weakly supervised learner which classifies a sample as speculative when its probability exceeds a predefined threshold.
This iterative learning approach is capable to perform hedge classification with similar accuracy as other approaches based on \acp{SVM}.
They conclude that their simple \ac{ML} based approach is capable to perform hedge classification and more complex \ac{ML} approaches are likely to perform even better.

\textcite{Ormandjieva:2007} present an approach which utilizes decision trees for the classification task.
Their quality model distinguishes among \textit{surface understanding} and \textit{conceptual understanding}.
Surface understanding means the literal meaning like how difficult or easy it is to understand a requirements documents' passages.
Whereas conceptual understanding aims for a passage's interpretation.
For example how difficult it is for a developer to implement a system by only reading the paragraph.
The dataset was obtained by manual reviews of four annotators.
Each annotator carefully read a set of requirements documents and classified its passages once with respect to surface understanding and once with respect to conceptual understanding.
The inter-rator agreement is indicated by Cohen's Kappa \parencite{Cohen:1960} and is 0.64 for conceptual understanding and 0.66 for surface understanding.
For the classification task each sentence of the requirements documents was \ac{POS} tagged and its syntax parsed using the Stanford Parser \parencite{Klein:2002}.
Then the number of occurrences of the indicators for little surface understanding were counted.
Based on that information a decision tree was constructed.
The authors mention that a \ac{NN} could maybe achieve better results.
However, due to the lack of data a decision tree is preferred.
The trained decision tree was capable to solve the classification task with 86.67\% accuracy.
\textcite{Ormandjieva:2007} conclude that it is indeed feasible to uncover faults related to surface understanding using a decision tree and see their quality model approved as suitable.

\textcite{Yang:2012} use \ac{CRF} to extract uncertainty cues from requirements documents.
They manually labeled several requirements documents for uncertainty cues and their corresponding scopes.
The detection problem was formulated as a labelling task of a sequence on token-level.
Each word of a sentence is assigned a class label indicating whether the word is the first word of the cue, inside a cue or not in a cue.
To additionally gather further information regarding the semantics of a cue keyword, more linguistic features were extracted.
Examples are the word lemma, \ac{POS} tag, \ac{POS} tags of the three neighboring words and grammatical relations.
Those features and the class label are used to train a classification model using the \ac{CRF} algorithm \parencite{Lafferty:2001}.
Further, a post-processing step is applied to to capture infrequent cues extracted from the training dataset.
In this step the cues are detected using string matching.
When a match occurs the sentence is classified as speculative.
In the conducted study multiple models with a different amount of features was trained and tested.
Speculative sentences were detected with 85.58\% precision and 77.65\% recall considering all features.
Regarding uncertainty scope identification the presented system performs worse: 54.37\% precision and 49.95\% recall were achieved.
They conclude that their implemented approach works well on uncovering speculative sentences and suggest that different supervised \ac{ML} techniques could be promising in solving this task.

\textcite{Stajner:2017} examine linguistic hedging in the monetary political domain.
They create two different datasets.
One includes transcribed \textit{debates} of different meetings of several committees.
The second dataset includes the meetings statement reports which are typically conducted at the end of a meeting and further referred to as \textit{decisions}.
The datasets were labeled \textit{speculative} or \textit{non-speculative} by three annotators.
The averaged pairwise Cohen's Kappa \parencite{Cohen:1960} was 0.56 for the debates and 0.61 for the decisions dataset.
The annotators marked phrases which indicate a speculative sentence in their opinion.
For the classification task a \ac{SVM} was trained on both datasets.
They used \ac{BoW}, the lists of speculation triggers or different combinations as input features for the \acp{SVM}.
The authors benchmarked their \ac{SVM} approach with a \ac{CNN} and the best performing system for the CoNLL-2010 shared task \parencite{Farkas:2010}.
The conclude that a general domain dataset can be used to train a model for a hedge classification task in this domain.
Further they showed that their \ac{SVM} approach including lists of speculation triggers performs as well as other state of the art systems in other domains and could outperform the \ac{CNN} classifier on the debates dataset.
