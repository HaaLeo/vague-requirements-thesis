\section{Dataset}
\label{chp:future_work:sec:dataset}
According to \textcite{Domingos:2012}, it is likely to obtain a better performing algorithm if it is trained on more data.
In the study we use a dataset consisting of $2,776$ entries, but only $21\%$ of them are labeled as vague.
This raises the question of whether a classifier can be trained to reliably classify requirements with that little amount of vague datapoints.

In order to address this concern, future research should consider to extend the contributed dataset and evaluate the impact of a bigger dataset in further studies.
Moreover, it would be interesting to see how the models behave with more and more data.
To evaluate this, the models can be trained incrementally with more data in each training step.
With this procedure one can examine whether there exists a "sweat spot" of the amount of data which leads to most performance gain.
