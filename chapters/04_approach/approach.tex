\chapter{Approach}
\label{chp:approach}
% (wie werde ich die Vagheit detektieren â†’ NN, Transformer Architektur) welche
% Grob vorstellen was ich machen werde:

% ich benutze mehrere NNs um Vagheit zu klassifizieren
% Eingaben, Ausgaben
% eher top Level

In this chapter the concrete approach is presented which is used to detect vague requirements.
To classify requirements as vague or not we leverage \acp{DNN}.
The result of each \ac{DNN} is evaluated using dedicated metrics.

Transformer-based approaches have turned out to be viable alternatives to traditional architectures based on \acp{RNN} for \ac{NLP} tasks like language translation \parencites{Gehring:2017}{Vaswani:2017}.
This showcases that transformers are capable of solving complex \ac{NLP} tasks.
Therefore, we base our approach on transformers to attempt to classify vague requirements.
In detail, our approach considers three different pre-trained models as basis.
Namely \ac{BERT} \parencite{Devlin:2018}, \ac{DistilBERT} \parencite{Sanh:2019} and \ac{ERNIE2.0} \parencite{Sun:2019a}.
The following sections outline which particular models are used.

\input{chapters/04_approach/sections/BERT}
\input{chapters/04_approach/sections/distilBERT}
\input{chapters/04_approach/sections/ERNIE2.0}
