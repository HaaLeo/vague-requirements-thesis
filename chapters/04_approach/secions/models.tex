\section{Models}
\label{chp:approach:sec:models}

Transformer based approaches have turned out to be viable alternatives to traditional architectures based on \acp{RNN} for \ac{NLP} tasks like language translation \parencites{Gehring:2017}{Vaswani:2017}.
This showcases that transformers are capable to solve complex \ac{NLP} tasks.
Therefore, we also leverage a transformer based approach to attempt to classify vague requirements.
In detail, our approach considers three different models.
Namely \ac{BERT} \parencite{Devlin:2018}, \ac{DistilBERT} \parencite{Sanh:2019} and \ac{ERNIE2.0} \parencite{Sun:2019a}.
This chapter presents the used models and their properties.

\Ac{BERT} is based on \textit{encoder} of the original transformer model of \textcite{Vaswani:2017}.
It is simply a stack of encoders.
There exist variants of \ac{BERT}: \ac{BERT}$_{base}$ and \ac{BERT}$_{large}$.
The first consists of 12 encoder layers, second of 24 which yields to 110 million parameters and 310 million respectively.
More trainable parameters usually lead to a better performance.
On the other hand
