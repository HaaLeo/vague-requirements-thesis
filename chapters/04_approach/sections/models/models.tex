\section{Models}
\label{chp:approach:sec:models}

Transformer based approaches have turned out to be viable alternatives to traditional architectures based on \acp{RNN} for \ac{NLP} tasks like language translation \parencites{Gehring:2017}{Vaswani:2017}.
This showcases that transformers are capable to solve complex \ac{NLP} tasks.
Therefore, we also leverage a transformer based approach to attempt to classify vague requirements.
In detail, our approach considers three different models.
Namely \ac{BERT} \parencite{Devlin:2018}, \ac{DistilBERT} \parencite{Sanh:2019} and \ac{ERNIE2.0} \parencite{Sun:2019a}.
This chapter presents the used models and their properties.

\input{chapters/04_approach/sections/models/subsections/BERT}
\input{chapters/04_approach/sections/models/subsections/distilBERT}
\input{chapters/04_approach/sections/models/subsections/ERNIE2.0}
