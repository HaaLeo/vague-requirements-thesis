\subsection{Cohen's Kappa}
\label{chp:fundamentals:sec:inter_rater_agreement:subsec:cohens_kappa}
Cohen's Kappa $\kappa$ was introduced by \textcite{Cohen:1960} in 1960.
He states the hypothesis that even if all raters are unaware of the correct answer and purely guessing, nevertheless some data points are congruent.
In his opinion random congruency should be considered by agreement statistics.
To tackle this issue he introduced the kappa statistics to account for the random agreement among two raters.
Similar to other correlation statistics, it can take values in the range from -1 to 1.
0 indicates the agreement obtained by random choice, whereas 1 represents perfect agreement.
The kappa calculation includes two quantities.
$P_o$ is the proportion of observed agreement of raters and $P_e$ is the proportion of rating agreement expected to be obtained by chance.
The overall formula of Cohen's $\kappa$ is then given by the following equation:

\begin{equation}\label{eq:Cohens_kappa}
    \kappa = \frac{P_o - P_e}{1 - P_e}
\end{equation}

Consider two raters, A and B respectively, assigning $N$ data points to $C$ categories.
$n_{c_i, c_j}$ indicates how many data points were assigned by rater $A$ to class $c_i$ and to $c_j$ by rater $B$.
$p_{A, c_i}$ represents the proportion of assignments that were assigned to class $c_i$ by rater $A$.
An overview for $C=3$ is given in \cref{tab:cohens_kappa_sample_definition}.

\begin{table}[htpb]
    \centering
    \begin{tabular}{l|l|c|c|c|c}
        \multicolumn{2}{c}{}&\multicolumn{3}{c}{B}&\\
        \cline{3-5}
        \multicolumn{2}{c|}{}&$c_1$&$c_2$&$c_3$&\multicolumn{1}{c}{Proportion}\\
        \cline{2-5}
        \multirow{3}{*}{A}& $c_1$ & $n_{c_1, c_1}$ & $n_{c_1, c_2}$ &$n_{c_1, c_3}$& $p_{A, c_1}$\\
        \cline{2-5}
        & $c_2$ & $n_{c_2, c_1}$ & $n_{c_2, c_2}$ &$n_{c_2, c_3}$&$p_{A, c_2}$\\
        \cline{2-5}
        & $c_3$ & $n_{c_3, c_1}$ & $n_{c_3, c_2}$ &$n_{c_3, c_3}$ & $p_{A, c_3}$\\
        \cline{2-5}
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{Proportion} & \multicolumn{1}{c}{$p_{B, c_1}$} & \multicolumn{1}{c}{$p_{B, c_2}$} & \multicolumn{1}{c}{$p_{B, c_3}$} & \multicolumn{1}{c}{$1$}\\
    \end{tabular}
    \caption[Cohen's Kappa notation overview]{Example confusion matrix.}\label{tab:cohens_kappa_sample_definition}
\end{table}

$P_o$ is then given by: %TODO check here for unwanted page breaks
\begin{equation}\label{eq:Cohens_kappa:p_o}
    P_o = \frac{\sum_{i=1}^{C} n_{c_i, c_i}}{N}
\end{equation}

Whereas the expected agreement by chance $P_e$ is calculated according to \cref{eq:Cohens_kappa:p_e}:
\begin{equation}\label{eq:Cohens_kappa:p_e}
    P_e = \sum_{i=1}^{C} p_{A, c_i} p_{B, c_i}
\end{equation}

An example for the case $C=2$ is illustrated in \cref{tab:cohens_kappa_sample_data}.
Here two raters rated 50 requirements as \textit{Good} or \textit{Bad}.

\begin{table}[htpb]
    \centering
    \begin{tabular}{l|l|c|c|c}
        \multicolumn{2}{c}{}&\multicolumn{2}{c}{A}&\\
        \cline{3-4}
        \multicolumn{2}{c|}{}&Good&Bad&\multicolumn{1}{c}{Proportion}\\
        \cline{2-4}
        \multirow{2}{*}{B}& Good & $20$ & $5$ & $0.5$\\
        \cline{2-4}
        & Bad & $10$ & $15$ & $0.5$\\
        \cline{2-4}
        \multicolumn{1}{c}{} & \multicolumn{1}{c}{Proportion} & \multicolumn{1}{c}{$0.6$} & \multicolumn{    1}{c}{$0.4$} & \multicolumn{1}{c}{$1$}\\
    \end{tabular}
    \caption[Cohen's Kappa sample data]{Example data of 2 raters assigning 50 data points to two categories.}\label{tab:cohens_kappa_sample_data}
\end{table}

Given \cref{tab:cohens_kappa_sample_data}, one can directly calculate $P_o = \frac{ 20 + 15 }{50} = 0.7$ and $P_e = 0.6 \cdot 0.5 + 0.4 \cdot 0.5 = 0.5$.
Now $P_o$ and $P_e$ are plugged in \cref{eq:Cohens_kappa} which yields a kappa $\kappa = \frac{0.7 - 0.5}{1 - 0.5} = 0.4$.
