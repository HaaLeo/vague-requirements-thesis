\subsection[Scott's Pi]{Scott's $\pi$}
\label{chp:fundamentals:sec:inter_rater_agreement:subsec:scotts_pi}
\textcite{Scott:1955} developed another inter-observer agreement metric.
It was introduced specifically to measure the agreement for survey research.
This field of research includes annotating textual entities with classes by different annotators which is a common use case in \ac{NLP}.
Its aim is to measure the agreement among raters' multiple responses which are classified in exclusive categories.
According to \textcite{Scott:1955}, the percent of answers which the raters agree on as well as the \textit{consistency index S} introduced by \textcite{Bennett:1954} are biased.
Under the assumption that annotators have the same distribution of answers, he introduced his index $\pi$, referred to as \textit{Scott's $\pi$}, which has the same formula as \hyperref[chp:fundamentals:sec:inter_rater_agreement:subsec:cohens_kappa]{Cohen's kappa} (\cref{eq:Cohens_kappa}).

\begin{equation}\label{eq:Scotts_pi}
    \pi = \frac{P_o - P_e}{1 - P_e}
\end{equation}

Here $P_o$ is again the observed percentage of agreement and $P_e$ is the percentage of agreement which can be expected merely by chance.
Similar to Cohen's kappa Scott's $\pi$ is limited to two raters.
The only difference of Scott's $\pi$ to Cohen's kappa is the calculation of the by chance expected agreement $P_e$.
In contrast to \textcite{Cohen:1960} who uses the squared geometric mean of marginal proportions, \textcite{Scott:1955} used their squared arithmetic mean.
Following the notation of \cref{tab:cohens_kappa_sample_definition} this yields:

\begin{equation}\label{eq:Scotts_pi:p_e}
    P_e = \sum_{i=1}^{C} (\frac{p_{A, c_i} + p_{B, c_i}}{2})^2
\end{equation}

Applying Scott's $\pi$ to the sample data listed in \cref{tab:cohens_kappa_sample_data} one obtains the same $P_o=0.7$ but a different $P_e = (\frac{0.5 + 0.6}{2})^2 + (\frac{0.4+0.5}{2})^2 = 0.505$.
Using $P_o$ and $P_e$, yields Scott's $\pi = \frac{0.7 - 0.505}{1-0.505} = 0.\overline{39}$.
