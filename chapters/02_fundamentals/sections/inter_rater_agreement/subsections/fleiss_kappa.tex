\subsection{Fleiss' Kappa}
\label{chp:fundamentals:sec:inter_rater_agreement:subsec:fleiss_kappa}
Cohen's kappa and Scott's $\pi$ focus only the agreement between two raters.
To overcome this limitation \textcite{Fleiss:1971} introduced another kappa statistics as a generalization of Scott's $\pi$ \parencite{Scott:1955}.
Consider $N$ datapoints, where each datapoint is assigned $n$ times to one of $C$ classes by different raters.
How many raters have assigned the $i$th datapoint to the $j$th class is indicated by $n_{ij}$.
$p_j$ is the proportion of all assignments which were made to the $j$th class and is calculated as:

\begin{equation}\label{eq:fleiss_pj}
    p_j = \frac{1}{Nn}\sum_{i=1}^N n_{ij}
\end{equation}

The agreement of $n$ raters on the $i$th datapoint, indicated by $P_i$, is the proportion of agreeing rater pairs out of the $n(n-1)$ possible pairs and given by the following:

\begin{equation}\label{eq:fleiss_Pi}
    \begin{aligned}
        P_i &= \frac{1}{n(n-1)} \sum_{j=1}^C n_{ij} (n_{ij}-1) \\
        &= \frac{1}{n(n-1)} \left[\left(\sum_{j=1}^C n_{ij}^2 \right) - n\right]\\
    \end{aligned}
\end{equation}

The mean of all $P_i$ then forms the overall agreement.

\begin{equation}\label{eq:fleiss_P_bar}
    \bar{P} = \frac{1}{N} \sum_{i=1}^N P_i
\end{equation}

The agreement achieved merely by chance $\bar{P_e}$ is similar to \textcite{Scott:1955} indicated by

\begin{equation}\label{eq:fleiss_P_e}
    \bar{P_e} = \sum_{j=1}^C p_j^2
\end{equation}

According to \textcite{Fleiss:1971}, the term $1-\bar{P_e}$ measures the agreement which can be achieved in extent to the agreement which is obtained by chance.
The actual agreement including the agreement by chance is represented by $\bar{P} - \bar{P_e}$.
Then the normalized kappa statistics is similar to \textcite{Cohen:1960} given by:

\begin{equation}\label{eq:fleiss_kappa}
    \kappa_{fleiss} = \frac{\bar{P}-\bar{P_e}}{1-\bar{P_e}}
\end{equation}

When applying Fleiss' kappa to the sample data of \cref{tab:cohens_kappa_sample_data} one can easily see that all $P_i$s are either $1$ when the raters agree or $0$ if one rater votes for "Good" and one for "Bad".
Considering this, \cref{eq:fleiss_P_bar} simplifies to $\bar{P} = \frac{20+15}{50} = 0.7$.
Since the expected agreement purely achieved by chance is calculated in the same way as for Scott's $\pi$, one immediately notices that for this test data the two metrics are equal $\kappa_{fleiss} = \pi = 0.\overline{39}$ like one would expect.
