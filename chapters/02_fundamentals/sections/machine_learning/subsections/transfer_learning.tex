\subsection{Transfer Learning}
\label{chp:fundamentals:sec:machine_learning:subsec:transfer_learning}
According to \textcite{Tan:2018}, deep learning suffers two main drawbacks.
Firstly the authors state that deep learning models depend heavily on the used training data.
The models require massive amounts of training data to learn the latent structure from the samples.
Further, the relationship of a model's size and the required training data scales linearly \parencite{Tan:2018}.

The second concern \textcite{Tan:2018} raise is insufficient training data.
This applies mainly in special domains where it is very hard to generate training samples.
For instance if a model uses a patient's data for training, it requires thousands of patients in order to be able to create an extensive dataset.

One approach to overcome these drawbacks is \textit{transfer learning}.
According to \textcite{Tan:2018} transfer learning \textit{"aims to improve the performance of [a] predictive function [...] for [a] learning task [...] by discover[ing] and transfer[ring] latent knowledge"}.
Further, they state that the dataset used to pre-train the model is often way bigger than the actual target dataset.
They distinguish among different types of transfer learning.
One is called \textit{network-based} transfer learning.
To apply this approach, one (partially) reuses a \ac{NN} which was pre-trained on a massive dataset in a specific source domain.
This approach assumes that the features which the \ac{NN} extracts are similar in the source and in the target domain.
Throughout this work, when we use the term "transfer learning" we refer to network-based transfer learning as defined by \textcite{Tan:2018}.
