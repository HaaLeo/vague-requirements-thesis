\subsection{Local Interpretable Model-Agnostic Explanations}
\label{chp:fundamentals:sec:machine_learning:subsec:LIME}
According to \textcite{Ribeiro:2016} \ac{ML} models are widely spread, although they mostly remain black boxes.
Further, they state that it is essential to be able to trust a model when it is used to make decisions.
However, it is very difficult for a human to trust a model which cannot explain its predictions.
To address this issue \textcite{Ribeiro:2016} introduce \ac{LIME}.

When one wants to apply \ac{LIME} on a sample $s$ to obtain an explanation for the prediction, it first samples datapoints near $s$ and weights them with their distance to $s$.
Then the predictions for the sampled datapoints are generated by the original model.
This labeled dataset is used to train an interpretable linear model which approximates the original one well near $s$.
Consequently, \ac{LIME} presumes that a complex model is linear on a local scale.
The explanations for the original model's prediction of sample $s$ are then obtained by using the interpretable model. \parencite{Ribeiro:2016}

An example is shown in \cref{fig:fundamentals:LIME}.
The prediction function of the original model is indicated by the light/dark blue background.
Obviously one cannot linearly approximate the original model's prediction function.
The red cross is our example sample $s$.
The samples generated by \ac{LIME} are indicated by the orange crosses and grey circles.
The weighting is indicated by the samples marker size.
The dashed line represents \ac{LIME}'s learned decision function which approximates the original model near $s$ but not globally.
The locally learned model is then used to assess the original model's prediction for $s$.
\begin{figure}[htpb]
    \centering
    % \def\svgwidth{\columnwidth}
    \input{figures/fundamentals/LIME.pdf_tex}
    \caption[LIME Example Data]{A toy example to explain \ac{LIME}'s functionality adopted from \textcite{Ribeiro:2016}.}\label{fig:fundamentals:LIME}
\end{figure}
