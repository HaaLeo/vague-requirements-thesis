\subsection{Local Interpretable Model-Agnostic Explanations}
\label{chp:fundamentals:sec:machine_learning:subsec:transfer_learning}
According to \textcite{Ribeiro:2016} \ac{ML} models are widely spread, although they mostly remain black boxes.
Further, they state that is essential to be able to trust a model when it used to make decisions.
However, it is very difficult for a human to trust a model which cannot explain its predictions.
To address this issue \textcite{Ribeiro:2016} introduce \ac{LIME}.

When one wants to apply \ac{LIME} for a sample $s$ to obtain an explanation for the prediction it first samples datapoints near $s$ and weights them with their distance to $s$.
Then the predictions for the sampled datapoints are generated by the original model.
This labeled dataset is used to train an interpretable linear model which approximates the original well near $s$.
Consequently, \ac{LIME} presumes that a complex model is linear on a local scale.
The explanations for the prediction of $s$ of the original model are then derived using the interpretable model. \parencite{Ribeiro:2016}
