\subsection{Average Precision}
\label{chp:fundamentals:sec:metrics:subsec:average_precision}

Due to the previously mentioned trade-off between recall and precision both of those metrics must be considered when optimizing a model for a task.
One metric which considers recall as well as precision is \ac{AP}.

\subsubsection{Definition}
\label{chp:fundamentals:sec:metrics:subsec:average_precision:definition}

\Textcite{Zhu:2004} defines it as the following:

\begin{equation}\label{eq:average_precision}
    AP = \sum_{i=1}^n {p(i)\Delta r(i)}
\end{equation}

Where $p(i)$ is the precision taking into account the first $i$ elements and $\Delta r(i)$ indicates the the recall change from the $i-1$th to the $i$th item.
With this metric the order of the sequence is crucial.
An algorithm which manages to sort a sequence of elements in a way that all relevant items are listed first achieves higher AP than an algorithm that performs poorly on the sorting task.

Let us consider the following example.
An algorithm was trained to return all relevant items from a dataset.
Given an unseen dataset of five items the model returns the following sorted sequence given in \cref{fig:metrics:average_precision:sample}.
The sequence is sorted in descending order, meaning the elements which are considered as most relevant by the algorithm are inserted first.

\begin{figure}[htpb]
    \centering
    \def\svgwidth{\columnwidth}
    \input{figures/metrics/Average_Precision.pdf_tex}
    \caption[Example Sequence]{A sorted example sequence returned by some algorithm.}\label{fig:metrics:average_precision:sample}
\end{figure}

Now we can calculate the \ac{AP} according to \cref{eq:average_precision}:
\begin{equation}
    \begin{aligned}
        AP &= \sum_{i=1}^n {p(i)\Delta r(i)}\\
        &= \frac{1}{1} \cdot \frac{1}{3} + \frac{1}{2} \cdot 0 + \frac{2}{3} \cdot \frac{1}{3} + \frac{3}{4} \cdot \frac{1}{3} + \frac{3}{5} \cdot 0\\
        &=  (\frac{1}{1} + \frac{2}{3}  + \frac{3}{4}) \cdot \frac{1}{3}\\
        &\approx 0.81
    \end{aligned}
\end{equation}

\subsubsection{Interpretation}
\label{chp:fundamentals:sec:metrics:subsec:average_precision:interpretation}
The overall interpretation of \ac{AP} is intuitive: The more relevant documents are ranked at the top of the returned sequence the higher will \ac{AP} be.
However, in contrast to precision and recall, it is more difficult to judge whether the retrieved value for \ac{AP} is "good" or "bad".
Should we consider a system which yields $\ac{AP}=0.66$ as good?
In this part we want to address this issue and provide a tangible interpretation of the \ac{AP} value.

First we want to take a look at different example sequences and their corresponding \ac{AP} values.
We then derive an intuitive explanation for these example sequences which helps to judge \ac{AP} in general.
Let us consider two example sequences \textit{sequence 1} and \textit{sequence 2} shown in \cref{fig:metrics:average_precision:interpreation:sample}.

\begin{figure}[htpb]
    \centering
    \def\svgwidth{\columnwidth}
    \input{figures/metrics/Average_Precision_Example.pdf_tex}
    \caption[Two Example Sequences]{Two example sequences taken from \textcite{Tapaswi:2012}}\label{fig:metrics:average_precision:interpreation:sample}
\end{figure}

For the first sequence we observe that \textit{every third} top ranked item is relevant whereas in the second sequence \textit{every second} item is.
We can now calculate the \ac{AP} for both sequences according to \cref{eq:average_precision} which yields $AP_1= \frac{1}{3}$ for the first sequence and $AP_2= \frac{1}{2}$ for the second one.
With the sequences above and their corresponding \acp{AP} $AP_1$ and $AP_2$ we can now conclude that an $AP=\frac{1}{j}$ indicates that every $j$th item is relevant \parencite{Tapaswi:2012}.
With this in mind an $\ac{AP}=0.66$ is rather good, because approximately every $1.5$th item among the top ranks is relevant, whereas with $AP=0.2$ only every fifth item is relevant.
