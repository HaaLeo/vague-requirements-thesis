\section{Metrics}
\label{chp:fundamentals:sec:metrics}

Usually different metrics are used to evaluate classification models.
This chapter defines required terms before the metrics are introduced in the following subchapters.

When introducing metrics for predictions it is good practice to consider the prediction labels as \textit{positives} and \textit{negatives} respectively.
Then the correctly predicted positives are referred to as \acp{TP} and the incorrectly positive classified items as \acp{FP}.
The same scheme applies to the negatives and they are therefore distinguished into \acp{TN} and \acp{FN}.
\Cref{fig:metrics:tp_vis} visualizes those groups. \parencite{Powers:2011}

In \cref{fig:metrics:tp_vis} an example algorithm retrieved three relevant items referred to as \acp{TP}.
The four items falsely classified as relevant are the \acp{FP}.
The missed relevant items are the \acp{FN}.
The algorithm falsely missed nine relevant items (\acp{FN}) in the example data of \cref{fig:metrics:tp_vis}.
The algorithm correctly did not select eleven irrelevant items (\acp{TN}).
We refer to the data shown in \cref{fig:metrics:tp_vis} when introducing the metrics in the following subsections.
\begin{figure}[htpb]
    \centering
    \def\svgwidth{\columnwidth}
    \input{figures/metrics/True_Positives.pdf_tex}
    \caption[Visualization of True Positives]{A visualization of \ac{TP}, \ac{FP}, \ac{TN} and \ac{FN}.}\label{fig:metrics:tp_vis}
\end{figure}

\input{chapters/02_fundamentals/sections/metrics/subsections/precision}
\input{chapters/02_fundamentals/sections/metrics/subsections/recall}
\input{chapters/02_fundamentals/sections/metrics/subsections/f1_score}
\input{chapters/02_fundamentals/sections/metrics/subsections/average_precision}
