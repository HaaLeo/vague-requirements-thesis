\chapter{Conclusion}
\label{chp:conclusion}
The modern software engineering process includes the specification of requirements.
Defining proper requirements which are understood by all stakeholders can be difficult.
Faults which occur during this phase cost time and money in subsequent process steps \parencite{Mendez:2016} and even result in sever project delay \parencite{Femmer:2014}.
To tackle these issues and offer possible solutions for them, the research field \ac{RE} emerged.
Until now, \ac{RE} mostly considered rule-based approaches which already lead to promising results.
Although in recent years \ac{ML} based approaches returned astonishing results on various \ac{NLP} tasks, there is rather little research in the domain of \ac{RE} ongoing using state of the art \ac{ML} approaches.
The aim of this thesis is to bridge this gap and evaluate whether and to what extent state of the art \ac{ML} based approaches are capable to to detect vague requirements.

Within this thesis we first discuss how different research fields define and use the term "vagueness" before we define what we consider a \textit{vague requirement} throughout this work.
As state of the art \ac{ML} models we use transformer-based models, namely \ac{BERT}, \ac{DistilBERT} and \ac{ERNIE2.0}.
All those models are trained with supervised learning.
This technique requires an annotated dataset which, to our knowledge, was not contributed by earlier research.
Therefore, with this thesis we contribute a dataset including $2776$ datapoints of which $589$ are labeled as vague and $2187$ are annotated not-vague.
The dataset is created partially via crowdsourcing and manual labeling by an expert.
We then use this dataset to apply transfer learning to the models.
This means that we take a pre-trained version of each model and train it on our domain specific dataset.
With the trained models we generate predictions on new unseen data which are used to calculate precision, recall, $F_1$ score and \ac{AP}.
The models' maximum values for these metrics are $45\%$ precision, $85\%$ recall, $50\%$ $F_1$ score and $46\%$ \ac{AP}.
Comparing these results with other rule based approaches yields that in this thesis introduced approaches perform worse.
In order to verify this conclusion we take a look at other transformer-based approaches from different domains.
All those approaches return rather poor results for the vanilla implementation of \ac{BERT} using a simple downstream classifier.
Most of the approaches are capable to significantly enhance the models' performance by using a larger dataset or a more sophisticated downstream classification strategy.

Our research shows that \ac{ML} approaches based on pre-trained transformers are no plug-and-play solutions to revolute the field of \ac{RE}.
But on the contrary, these approaches perform even worse than traditional approaches.
We conclude that transformers in their examined versions are worse in detecting bad requirements than traditional approaches.
Although the result appears sobering at first glance, it offers interesting follow-up questions as opportunity for future research which can use the presented results as baseline.
It remains exciting, how transformers and future \ac{ML} approaches in general will perform on \ac{NLP} tasks of the \ac{RE} domain.
