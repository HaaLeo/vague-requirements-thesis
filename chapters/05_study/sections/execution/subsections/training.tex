\subsection{Training}
\label{chp:study:sec:execution:subsec:training}
With the found hyperparameters we now train our models on the datasets.
The training phase is split into two separate runs each using a different dataset.

\subsubsection{First Run}
\label{chp:study:sec:execution:subsec:training:first_run}
% Execution with D_{crowd}
In the first run we train all three models on $D_{crowd_{train}}$ with the best hyperparameter configuration found by grid search executed with $D_{crowd_{train}}$.
After that we use the trained models to obtain predictions for the test dataset $D_{crowd_{test}}$.
Considering the correctly as vague classified requirements as \acp{TP} we then calculate the metrics introduced in \cref{chp:fundamentals:sec:metrics}.
The resulting metrics indicate that the models performed rather poorly on the classification task.
For instance, no model achieves an $F_1$ score greater than $0.6$ or an \ac{AP} greater than $0.5$.
Instead of listing all results here, we refer to the \cref{chp:study:sec:results} where the complete results of all runs are shown comprehensively.

% Why we create D_{manual}
Our trained models perform rather poorly in correctly classifying the requirements of $D_{crowd_{test}}$.
According to \textcite{Domingos:2012} a \textit{"dumb algorithm with lots and lots of data [can beat] a clever one with modest amounts of it"}.
It follows, that more data leads to better performing algorithms and \ac{ML} models.
Since we discarded almost half of all requirements when creating $D_{crowd}$, raises the question of whether our models have had enough data to train properly.
To encounter this concern and to include all available requirements in the dataset we relabeled all discarded requirements like we described in \cref{chp:study:sec:study_objects:dataset_creation:manual_labeling}.

\subsubsection{Second Run}
\label{chp:study:sec:execution:subsec:training:second_run}

% Execution with D_{all}
For the second run we now use the more extensive dataset $D_{all}$.
We again split it in a training partition $D_{all_{train}}$ and test dataset $D_{all_{test}}$ with the proportion $1\mathrm{:}9$ preserving the distribution of vague requirements for both.
We then train all of our models on the new dataset $D_{all_{train}}$ with the hyperparameters obtained by the preceding grid search.
Analogically to the first run, we obtain predictions for the dataset $D_{all_{test}}$ and then calculate the metrics for evaluation considering the correctly as vague classified requirements as \acp{TP}.
