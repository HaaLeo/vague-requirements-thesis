
\subsection{Grid Search}
\label{chp:study:sec:execution:subsec:gridsearch}
Training \ac{ML} models involves the setting of hyperparameter.
Those have a direct impact on the performance of a trained model and the user must set them appropriately to optimize the learning routine and with that the model \parencite{Claesen:2015}.
We perform a grid search to find a good parameter combination for each model.
We base the parameter grid for each model on the corresponding recommendations of the authors.
Further, we take a re-sampling strategy of our dataset into account for the grid search, because we deal with imbalanced data.

\subsubsection{Parameter Grid}
\label{chp:study:sec:execution:subsec:gridsearch:parameter_grid}
All our models are transformer based and involve multiple hyperparameters.
Transformer based models have the \textit{maximum length} of a sequence as hyperparameter.
Input sequences are truncated when they exceed this limit and are padded with 0 if they are shorter than this parameter.
Due to our limited dataset we want to perform $k$-fold cross validation.
In order to determine how many \textit{folds} we want to use we add this as parameter to our grid as well.
For the training of the models we use an imbalanced dataset.
To take account for that imbalance we consider different \textit{re-sampling} strategies.
\textit{Random down-sampling} randomly discards datapoints of the majority class to create a balanced dataset for the training phase.
\textit{Random up-sampling} achieves the same by randomly duplicating requirements of the minority class.
The parameters for \textit{learning rate}, \textit{epochs} and \textit{batch size} follow the the recommendations of the base models' authors.
Since all models are transformer based the parameter grids are similar.
The parameter grid for \ac{BERT} and \ac{DistilBERT} is shown in \cref{tab:study:execution:paramter_grid:BERT}.
\begin{table}[htpb]
    \centering
    \begin{tabular}{ p{3.5cm} p{1.5cm} p{2cm} p{1.5cm} p{1.6cm} p{1.5cm} }
        \toprule
         Re-sampling & Folds & Learning Rate & Epochs & Max Length & Batch Size \\
        \midrule
            \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item {random

                down-sampling}
                \item {random

                up-sampling}
            \end{itemize}
            &\begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item 4
                \item 8
            \end{itemize}
            & \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item $1\mathrm{e}{-06}$
                \item $5\mathrm{e}{-06}$
                \item $1\mathrm{e}{-05}$
                \item $5\mathrm{e}{-05}$
            \end{itemize}
            & \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item 1
                \item 2
                \item 3
            \end{itemize}
            & \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item 64
                \item 128
            \end{itemize}
            & \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item 16
                \item 32
            \end{itemize}\\
        \bottomrule
    \end{tabular}
    \caption[Parameter Grid for \ac{BERT} and \ac{DistilBERT}]{Parameter grid for \ac{BERT} and \ac{DistilBERT}.}\label{tab:study:execution:paramter_grid:BERT}
\end{table}

The parameter grid for \ac{ERNIE2.0} is similar, however \textcite{Sun:2019a} recommend different learning rates.
This leads to the parameter grid for \ac{ERNIE2.0} shown in \cref{tab:study:execution:paramter_grid:ERNIE2.0}
\begin{table}[htpb]
    \centering
    \begin{tabular}{ p{3.5cm} p{1.5cm} p{2cm} p{1.5cm} p{1.6cm} p{1.5cm} }
        \toprule
         Re-sampling & Folds & Learning Rate & Epochs & Max Length & Batch Size \\
        \midrule
            \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item {random

                down-sampling}
                \item {random

                up-sampling}
            \end{itemize}
            &\begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item 4
                \item 8
            \end{itemize}
            & \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item $2\mathrm{e}{-05}$
                \item $3\mathrm{e}{-05}$
                \item $4\mathrm{e}{-05}$
                \item $5\mathrm{e}{-05}$
            \end{itemize}
            & \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item 1
                \item 2
                \item 3
            \end{itemize}
            & \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item 64
                \item 128
            \end{itemize}
            & \begin{itemize}[noitemsep,topsep=0pt,leftmargin=15pt]
                \item 16
                \item 32
            \end{itemize}\\
        \bottomrule
    \end{tabular}
    \caption[Parameter Grid for \ac{ERNIE2.0}]{Parameter grid for \ac{ERNIE2.0}.}\label{tab:study:execution:paramter_grid:ERNIE2.0}
\end{table}

\subsubsection{Grid Search Execution}
\label{chp:study:sec:execution:subsec:gridsearch:execution}

Now we perform grid search using the crowdsourced dataset $D_{crowd}$.
For that we split $D_{crowd}$ and used $90\%$ of the data for training and training evaluation whereas the remaining $10\%$ of the data are merely used for the final test of the trained models.
We ensure that the resulting datasets $D_{crowd_{train}}$ and $D_{crowd_{test}}$ consist of the same proportion of vague datapoints of approximately $19\%$.
Finding a suitable learning rate is very challenging task, \textcite{Zeiler:2012} goes even further and states \textit{"[d]etermining a good learning rate becomes more of an art than science for many problems"}.
However, \textcite{Smith:2018} introduced the 1cycle learning rate schedule to dynamically adjust the learning rate during the training phase.
This policy achieved remarkable results in his experiments.
We use this approach and set the initial learning rate to the value of the corresponding parameter grid.
For a detailed description of this approach we refer to \citeauthor{Smith:2018}'s original paper \parencite{Smith:2018}.

\subsubsection{Found Hyperparameters}
\label{chp:study:sec:execution:subsec:gridsearch:execution}

Performing the grid search with the given parameter grids the parameter combinations which achieved the best $F_1$ score are listed in \cref{tab:study:execution:grid_search:results}.
All three models achieved a $F_1$ score around $0.5$.
\begin{table}[htpb]
    \centering
\begin{tabular}{l | p{2.9cm} l p{1.5cm} l p{1.6cm} l }
        \toprule
         Model & Re-sampling & Folds & Learning Rate & Epochs & Max Length & Batch Size \\
        \midrule
        \ac{BERT} & random\newline down-sampling & 4 & $1\mathrm{e}{-05}$ & 2 & 128 &16\\
        \ac{DistilBERT} & random\newline down-sampling & 4 & $5\mathrm{e}{-06}$ & 1 & 64 &32\\
        \ac{ERNIE2.0} & random\newline down-sampling & 4 & $4\mathrm{e}{-05}$ & 2 & 128 &32\\
        \bottomrule
    \end{tabular}
    \caption[Grid Search Results]{The best parameter combinations per model found with grid search.}\label{tab:study:execution:grid_search:results}
\end{table}
