\section{Results}
\label{chp:study:sec:results}
In this chapter we present the results of the executed study which we base the later discussion upon.
Specifically, we state the calculated metrics of the two runs.

First we look at the metrics of the first run in which we use the crowdsourced dataset $D_{crowd}$.
Here all three models achieve mostly similar metrics when classifying $D_{crowd_{test}}$.
All metrics are listed in \cref{tab:study:results:first_run} for each model.
\begin{table}[htpb]
    \centering
    \begin{tabular}{l | l l l l }
        \toprule
         Model & Precision & Recall & $F_1$ Score & Average Precision \\
        \midrule
        \ac{BERT} & $0.36$ & $0.82$ & $0.51$ & $0.50$\\
        \ac{DistilBERT} & $0.31$ & $0.79$ & $0.45$ & $0.47$\\
        \ac{ERNIE2.0} & $0.48$ & $0.52$ & $0.50$ & $0.47$\\
        \bottomrule
    \end{tabular}
    \caption[Study Results on Crowdsourced Dataset]{The results for the crowdsourced dataset $D_{crowd_{test}}$.}\label{tab:study:results:first_run}
\end{table}

As we describe in \cref{chp:study:sec:execution}, we performed a second run similar to the first one.
The only difference is that the more extensive dataset $D_{all_{train}}$ was used for the training and $D_{all_{test}}$ to calculate the resulting metrics correspondingly.
The results of this second run are shown in \cref{tab:study:results:second_run}.
\begin{table}[htpb]
    \centering
    \begin{tabular}{l | l l l l }
        \toprule
         Model & Precision & Recall & $F_1$ Score & Average Precision \\
        \midrule
        \ac{BERT} & $0.45$ & $0.53$ & $0.48$ & $0.46$\\
        \ac{DistilBERT} & $0.33$ & $0.71$ & $0.48$ & $0.46$\\
        \ac{ERNIE2.0} & $0.35$ & $0.85$ & $0.50$ & $0.43$\\
        \bottomrule
    \end{tabular}
    \caption[Study Results on Complete Dataset]{The results for the dataset $D_{all_{test}}$.}\label{tab:study:results:second_run}
\end{table}
