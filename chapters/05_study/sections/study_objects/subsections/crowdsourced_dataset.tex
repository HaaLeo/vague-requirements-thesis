\subsection{Crowdsourced Dataset}
\label{chp:study:sec:study_objects:crowdsourced_dataset}

% Introduction
The first approach to label the requirement dataset is to utilize crowdsourcing.
Using crowdsourcing to generate annotations raises the concern that annotators may have an insufficient level of skill to fulfill the task \parencite{Quinn:2011}.
Moreover, it is possible that they are even biased \parencite{Kittur:2008}.
Therefore, the quality of a dataset which was generated with crowdsourcing is not guaranteed.

% Description of the pre-tests
\subsubsection{Pre-Test Setup}
In order to address this issues and assess the quality of such a crowdsourced dataset we perform preliminary tests.
We crowdsource several small batches including 102 requirements as well as ask two experts $E1$ and $E2$ to label them.
All participants are asked to annotate requirements whether they are vague or not following the definition introduced in \cref{chp:fundamentals:sec:vagueness}.
In the first crowdsourced batch, referred to as $B_{C1}$, each requirement is labeled by two annotators, the second one named $B_{C2}$ is labeled by three.
The expert batches are referred to as $B_{E1}$ and $B_{E2}$ respectively.
Afterwards the \acp{IRA} within the crowdsourced batches and among the experts are calculated.
We choose the \hyperref[chp:fundamentals:sec:inter_rater_agreement:subsec:free_marginal_multirater_kappa]{\textit{free-marginal multirater kappa}} as \ac{IRA}, because raters do not know beforehand how many requirements are vague.
All four labeled batches are summarized in \cref{tab:study:objects:crowdsourcing:batches}.
\begin{table}[htpb]
    \centering
    \begin{tabular}{l l}
        \toprule
        Batch & Description \\
        \midrule
        $B_{C1}$ & Two assignments per requirement via crowdsourcing\\
        $B_{C2}$ & Three assignments per requirement via crowdsourcing\\
        $B_{E1}$ & One assignment per requirement by expert $E1$ \\
        $B_{E2}$ & One assignment per requirement by expert $E2$ \\
        \bottomrule
    \end{tabular}
    \caption[Overview of test batches]{An overview of all test batches.}\label{tab:study:objects:crowdsourcing:batches}
\end{table}

% Result of the pretests
% Todo: Is this paragraph necessary
\subsubsection{\Ac{IRA} per Batch}
We calculate the \ac{IRA} according to its definition in \cref{chp:fundamentals:sec:inter_rater_agreement:subsec:free_marginal_multirater_kappa} and observe that the results indicate low agreement within each batch.
The agreement between the raters which contributed to a batch $B$ is indicated $IRA(B)$.
The two experts achieved an agreement of $0.47$.
The first crowdsourced batch $B_{C1}$ has an \ac{IRA} of $0$ which indicates no agreement at all.
On the batch $B_{C2}$ the annotators have an \ac{IRA} of $0.29$.
These low values for the \ac{IRA} within the crowdsourced batches as well as among the experts indicates the difficulty of judging whether a requirement is vague or not, it \textit{does not} indicate the overall labels' quality.
The values are listed in \cref{tab:study:objects:crowdsourcing:batches:IRA}.
\begin{table}[htpb]
    \centering
    \begin{tabular}{l l}
        \toprule
        Batch $B$ & $IRA(B)$ \\
        \midrule
        $B_{C1}$ & $0$\\
        $B_{C2}$ & $0.29$\\
        $B_{E1} \cup B_{E2}$ & $0.47$ \\
        \bottomrule
    \end{tabular}
    \caption[Inter rater agreement within the batches]{The \ac{IRA} within each batch.}\label{tab:study:objects:crowdsourcing:batches:IRA}
\end{table}

% What is high quality
\subsubsection{Dataset Quality Assurance}
When generating a dataset we want to ensure a high quality, meaning it is similar to a dataset generated by experts.
We use the \ac{IRA} to measure the similarity of two datasets.
The more same requirements are labeled equally in two different datasets, the more similar are those datasets.
Consequently, a higher \ac{IRA} is an indicator for a high quality dataset.
However, it remains unclear what is an appropriate threshold for the \ac{IRA} to be considered as \textit{sufficiently high}.
Although \textcite{Landis:1977} introduced a fixed threshold, others state doubts and even argue that inevitably, those are arbitrary \parencites{Dunn:1989}{Brennan:1992}.
We indicate the \ac{IRA} between two datasets $Q$ and $P$ by $IRA(Q, P)$.
For our experiment we set the baseline for sufficient similarity to the \ac{IRA} between the two experts $E1$ and $E2$.
With this we can define the following sufficient \textit{quality condition}:
\begin{description}
    \item[$QC$]: Given a dataset $Q$ and a dataset $P_E$ which was aggregated from two separate expert datasets $P_{E1}$ and $P_{E2}$,\\
    $Q$ is of sufficient quality if $IRA(Q, P_E) \ge IRA(P_{E1}, P_{E2})$ holds.
\end{description}

\subsubsection{From Batch to Dataset}
% Description of majority label approach
To check whether a crowdsourced batch fulfills condition $QC$, we calculate the \ac{IRA} between the crowdsourced batches $B_{C1}$ and $B_{C2}$ and the expert batches $B_{E1}$ and $B_{E2}$.
To do that we treat each crowdsourcing batch as one rater by accumulating the different raters of a batch to a single one.
According to \textcite{Nowak:2010}, using the majority vote of multiple raters to generate a single label can reduce the noise of a dataset and consequently increase its quality.
Therefore, we want to compare the majority vote of the batches $B_{C1}$ and $B_{C2}$ with the expert batches.

% How is the majority label built?
Each requirement of $B_{C2}$ was labeled by three annotators.
Therefore, it is easy to calculate the majority vote for each requirement and we can introduce dataset $D_{C2}$, in which each requirement is assigned the label with the most votes of the raters of $B_{C2}$.
We cannot apply this procedure to $B_{C1}$, because it was labeled by an even number of raters.
Since we prefer the quality of the dataset to its size, we decide to drop all 51 requirements of $B_{C1}$ where the raters do not agree upon.
The resulting dataset is referred to as $D_{C1}$.
We proceed in the same way for two expert batches $B_{E1}$ and $B_{E2}$ and call the resulting dataset $D_{E}$.
Notice here that the two batches $B_{E1}$ and $B_{E2}$ are datasets themselves, because each of them only includes one label per requirement.

\subsubsection{Results}
% Result of IRA of crowdsourcing vs. experts.
We now calculate the \ac{IRA} between the new crowdsourcing datasets $D_{C1}$ and $D_{C2}$ and the experts' datasets $B_{E1}$, $B_{E2}$ and $D_{E}$.
The results are shown in \cref{tab:study:objects:crowdsourcing:batches:IRA:majority}.
\begin{table}[htpb]
    \centering
    \begin{tabular}{l l l}
        \toprule
        Dataset $Q$ & Dataset $P$ & $IRA(Q, P)$ \\
        \midrule
        $D_{C1}$ & $B_{E1}$ & $0.61$\\
        $D_{C1}$ & $B_{E2}$ & $0.26$\\
        $D_{C1}$ & $D_{E}$ & $0.57$\\
        $D_{C2}$ & $B_{E1}$ & $0.39$\\
        $D_{C2}$ & $B_{E2}$ & $0.14$\\
        $D_{C2}$ & $D_{E}$ & $0.36$\\
        \bottomrule
    \end{tabular}
    \caption[Inter rater agreement between crowdsourcing datasets and expert datasets]{The \ac{IRA} between the crowdsourcing datasets and the expert datasets.}\label{tab:study:objects:crowdsourcing:batches:IRA:majority}
\end{table}

From the table above one can immediately see that the raters of the crowdsourcing agree severely more with expert $E1$ than with $E2$.
Looking at the labels of $B_{E2}$ it is noticeable that $E2$ annotates 52 of the 102 requirements as vague, whereas the raters of all other batches tend to label a requirement as not-vague.
Therefore, the agreement with $B_{E2}$ is relatively low.
Further, it seems that the approach of dropping ties in $D_{C1}$ leads to a better agreement to the experts than building the majority vote upon an odd number of raters ($D_{C2}$).
Wse see that $D_{C1}$ highly agrees with with $B_{E1}$ and $D_{E}$ and it agrees moderately to $B_{E2}$.
Therefore, and because $D_{E}$ is an aggregation of $B_{E1}$ and $B_{E2}$ we want to check our condition $QC$
\begin{equation}
    \begin{aligned}
        IRA(D_{C1}, D_E) &\ge IRA(B_{E1}, B_{E2})\\
        0.57 &\ge 0.47
    \end{aligned}
\end{equation}

which obviously holds.
Consequently, a dataset which is created with the same procedure as $D_{C1}$ will have sufficient quality.

\subsubsection{The Dataset}
We choose to generate our dataset like the test dataset $D_{C1}$ despite the fact that many requirements will be discarded.
We crowdsourced all 2776 requirements of \citeauthor{Kummeth:2020}'s dataset \parencite{Kummeth:2020}.
The raters agree on $1548$ requirements, on $1228$ they disagree respectively.
$288$ requirements of the $1548$ are labeled as \textit{vague}, the remaining $1260$ are annotated \textit{not-vague}.
We refer to this crowdsourced dataset as $D_{crowd}$ in the following.
One point to notice is that $D_{crowd}$ is very unbalanced, only $\frac{288}{1548} \approx 19\%$ of the dataset's requirements is labeled vague.
