\chapter{Relation to Existing Evidence}
\label{chp:relation_to_existing_evidence}
From the previous section we see that our models perform rather poorly compared to traditional approaches.
In this section we want to introduce approaches which also use transformer based models and discuss differences and similarities of their approaches and ours.

\section{BERT Post-Training for Review Reading Comprehension}
\Textcite{Xu:2019} use \ac{BERT} for \ac{ASC} on product reviews.
Their derived \ac{ASC} model is based on \ac{BERT} and takes as input a \textit{review} and an \textit{aspect}.
The model then outputs the passed aspect's polarity in the review.
Possible values for the polarity are positive, neutral and negative.
For instance should the model output "positive" for the aspect "Hard drive" and the review "This laptop has a lightning fast hard drive, but its graphics are really bad.", but "negative" for the same review with the aspect "graphics".
They examine reviews from two domains, namely restaurants and laptops
Their laptop dataset contains $3845$ review sentences and $3012$ aspects whereas the restaurant dataset is slightly smaller.
It contains $2676$ sentences and $2365$ aspects.
They train plain \ac{BERT} and a custom version of \ac{BERT} on both restaurant reviews and laptop reviews.
The custom version includes a post-training on the extensive dataset which consists of ($100,000$+) sentences.
Using vanilla \ac{BERT} they achieved an accuracy of $75.29\%$ and macro $F_1$ score of $71.91\%$ for the laptop domain and even better for the restaurant domain.
However, the results obtained using plain \ac{BERT} are the \textit{worst} of their study.
The post-trained model achieves up to $84.95\%$ accuracy and $76.96\%$ macro $F_1$ score.
The authors conclude that their post-training technique is a legitimate technique to boost \ac{BERT} based models.
Further, their initial smaller dataset significantly limited \ac{BERT}'s performance gain.
Since their initial dataset is even larger than our dataset $D_{all}$, this indicates that our results may are limited by the dataset's capacity, too.

\section{Analysis of Propaganda in News Article}
\Textcite{Martino:2019} use a model based on \ac{BERT} to classify sentences whether they are propaganda or not.
They first define $18$ different \textit{propaganda techniques}.
With this definitions they annotate $7,485$ of $21,230$ sentences to indicate which propaganda techniques each sentence contains.
Their dataset is also quite imbalanced with only $35\%$ of the sentences containing propaganda.
They train different models on the dataset.
Their baseline model is similar to our \ac{BERT} based model.
They use the pre-trained \ac{BERT} version and feed its [CLS] token in a downstream feed forward \ac{NN} for binary classification.
With this model they achieve a precision of $63.20\%$, a recall of $53.16\%$ and a $F_1$ score of $57.74\%$.
In contrast to the baseline model, their newly proposed \textit{multi-granularity network} additionally incorporates all output tokens of \ac{BERT}.
This new model achieved $60.41\%$ precision, $61.58\%$ recall and $60.98\%$ $F_1$ score.
The authors conclude that the use of all output tokens can be utilized to increase a model's performance.
Since we only use as simple downstream \ac{NN}, the usage of a more complex classifier which leverages all output tokens could increase our models' performance as well.

\section[Target-Dependent Sentiment Classification With BERT]{Target-Dependent Sentiment Classification With \ac{BERT}}
\Textcite{Gao:2019} use \ac{BERT} to derive different models for aspect based sentiment analysis whose aim is to predict the polarity of a sentence's phrase.
They use the vanilla implementation of \ac{BERT} as baseline and use a dataset which consists of over $3,000$ restaurant as well as $3,000$ laptop reviews.
The baseline model again only uses the [CLS] token which is the input for a downstream \ac{NN}.
The more complex model uses the more output token of \ac{BERT}.
In addition to the [CLS] token, it also uses the output tokens of the sentence's target aspect.
Those token are then aggregated by an max-pooling layer before and then passed to a fully connected layer.
The authors obtained with vanilla \ac{BERT} a macro $F_1$ score of about $69\%$.
Their proposed model which uses more output token is able to boost the performance up to $79\%$ indicating that more complex downstream classifier can enhance the performance significantly.

\section{PatentBERT}
Another transformer based approach is introduced by \textcite{Lee:2019}.
They use \ac{BERT} to classify patents and call it PatentBERT.
In their work the authors use the vanilla \ac{BERT} with hyperparameters recommended by \textcite{Devlin:2018} and only use the [CLS] output token for the downstream classification.
The model is trained with different portions of overall $3,050,615$ patents.
Although PatentBERT was trained on a massive dataset, this rather simple approach achieves $32.19\%$ precision, $86.06\%$ recall and a $F_1$ score of $46.85\%$.
This indicates that an extensive dataset is not sufficient to significantly enhance the performance of a model.

\section{Smella}
When we compare our results from \cref{chp:study:sec:results} with a rule-based approach, one immediately observes that our models perform rather poorly.
One example is Smella, developed by \textcite{Femmer:2017}.
It uncovers \textit{requirement smells} with a precision of $59\%$ and a recall of $82\%$ whereas our \ac{BERT} based model achieves same recall, but only a precision of $0.36$.
In contrast to our \textit{vague requirements} Smella's objective is to identify requirement smells.
Although these objectives are not identical, they are very similar.
Therefore, we can follow that Smella as an example and rule based approaches in general are severely better in identifying poor requirements.
This is a very interesting finding, because this indicates that very sophisticated rule-based approaches currently outperform simple transformer-based \ac{ML} approaches.

\section{Summary}
All previously presented approaches try to solve different classification problems using variants of \ac{BERT}.
Most authors use the vanilla version of \ac{BERT} as baseline and observe that this implementation performs rather poorly compared to altered and complexer versions.
In \cref{tab:relation_to_existing_evidence:overview} we first list all the results of the vanilla implementation based on \ac{BERT} and the results of \textcite{Femmer:2017}.
Below that, the results our transformer based models are shown.

\begin{table}[htpb]
    \centering
    \begin{tabular}{l | l l l l l}
        \toprule
         Authors & Accuracy& Precision & Recall & (Macro) $F_1$ Score  \\
        \midrule
        \Textcite{Xu:2019} & $0.75$ & & & $0.72$  \\
        \Textcite{Martino:2019} & $0.63$ & $0.53$ & $0.58$  \\
        \Textcite{Gao:2019} &&&& $0.69$  \\
        \Textcite{Lee:2019} & $0.32$ & $0.86$ & $0.47$  \\
        \midrule
        \Textcite{Femmer:2017} && $0.59$ &$0.82$ &  \\
        \midrule
        \ac{BERT} & & $0.45$ & $0.53$ & $0.48$ \\
        \ac{DistilBERT} & & $0.33$ & $0.71$ & $0.48$ \\
        \ac{ERNIE2.0} & & $0.35$ & $0.85$ & $0.50$\\
        \bottomrule
    \end{tabular}
    \caption[Metrics of Related Approaches]{Metrics of related approaches.}\label{tab:relation_to_existing_evidence:overview}
\end{table}

From the table above we conclude that the baseline \ac{BERT} based models perform rather bad on their tasks.
Further, the rule-based approach introduced by \textcite{Femmer:2017} clearly outperforms our vanilla transformer based models.
Since the results of our implementations are similar to the baseline models of other transformer based approaches, we consider our results as plausible.
