\chapter{Relation to Existing Evidence}
\label{chp:relation_to_existing_evidence}
From the previous section we see that our models perform rather poorly compared to traditional approaches.
In this section we want to introduce approaches which also use transformer based models and compare their results with ours.

\Textcite{Xu:2019} use \ac{BERT} for \ac{ASC} on product reviews.
Their derived \ac{ASC} model is based on \ac{BERT} and takes as input a \textit{review} and an \textit{aspect}.
The model then outputs the givens aspect's polarity in the review.
Possible values for the polarity are positive, neutral and negative.
For instance should the model output "positive" for the aspect "Hard drive" and the review "This laptop has a lightning fast hard drive, but its graphics are really bad.", but "negative" for the same review with the aspect "graphics".
They examine reviews from two domains, namely restaurants and laptops
Their laptop dataset contains $3845$ review sentences and $3012$ aspects whereas the restaurant dataset is slightly smaller.
It contains $2676$ sentences and $2365$ aspects.
They train plain \ac{BERT} and a custom version of \ac{BERT} on both restaurant reviews and laptop reviews.
The custom version includes a post-training on the extensive dataset which consists of ($100,000$+) sentences.
Using vanilla \ac{BERT} they achieved an accuracy of $75.29\%$ and Macro-$F_1$ score of $71.91\%$ for the laptop domain and even better for the restaurant domain.
However, the results obtained using plain \ac{BERT} are the \textit{worst} of their study.
The post-trained model achieves up to $84.95\%$ accuracy and $76.96\%$ Macro-$F_1$ score.
The authors conclude that their post-training technique is a legitimate technique to boost \ac{BERT} based models.
Further, their initial and limited dataset significantly limited \ac{BERT}'s performance gain.
Since their initial dataset is even larger than our dataset $D_{all}$, this indicates that our results may are limited by the dataset's capacity, too.
