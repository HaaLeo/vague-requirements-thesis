\section{Transformer Based Approaches}
\label{chp:relation_to_existing_evidence:sec:transformer_based_approaches}

\Textcite{Xu:2019} use \ac{BERT} for \ac{ASC} on product reviews.
Their derived \ac{ASC} model is based on \ac{BERT} and takes as input a \textit{review} and an \textit{aspect}.
The model then outputs the passed aspect's polarity in the review.
Possible values for the polarity are positive, neutral and negative.
For instance should the model output "positive" for the aspect "Hard drive" and the review "This laptop has a lightning fast hard drive, but its graphics are really bad.", but "negative" for the same review with the aspect "graphics".
They examine reviews from two domains, namely restaurants and laptops
Their laptop dataset contains $3845$ review sentences and $3012$ aspects whereas the restaurant dataset is slightly smaller.
It contains $2676$ sentences and $2365$ aspects.
They train plain \ac{BERT} and a custom version of \ac{BERT} on both restaurant reviews and laptop reviews.
The custom version includes a post-training on the extensive dataset which consists of ($100,000$+) sentences.
Using vanilla \ac{BERT} they achieved an accuracy of $75.29\%$ and macro $F_1$ score of $71.91\%$ for the laptop domain and even better for the restaurant domain.
However, the results obtained using plain \ac{BERT} are the \textit{worst} of their study.
The post-trained model achieves up to $84.95\%$ accuracy and $76.96\%$ macro $F_1$ score.
The authors conclude that their post-training technique is a legitimate technique to boost \ac{BERT} based models.
Further, their initial smaller dataset significantly limited \ac{BERT}'s performance gain.
Since their initial dataset is even larger than our dataset $D_{all}$, this indicates that our results may are limited by the dataset's capacity, too.

\Textcite{Martino:2019} use a model based on \ac{BERT} to classify sentences whether they are propaganda or not.
They first define $18$ different \textit{propaganda techniques}.
With this definitions they annotate $7,485$ of $21,230$ sentences to indicate which propaganda techniques each sentence contains.
Their dataset is also quite imbalanced with only $35\%$ of the sentences containing propaganda.
They train different models on the dataset.
Their baseline model is similar to our \ac{BERT} based model.
They use the pre-trained \ac{BERT} version and feed its [CLS] token in a downstream feed forward \ac{NN} for binary classification.
With this model they achieve a precision of $63.20\%$, a recall of $53.16\%$ and a $F_1$ score of $57.74\%$.
In contrast to the baseline model, their newly proposed \textit{multi-granularity network} additionally incorporates all output tokens of \ac{BERT}.
This new model achieved $60.41\%$ precision, $61.58\%$ recall and $60.98\%$ $F_1$ score.
The authors conclude that the use of all output tokens can be utilized to increase a model's performance.
Since we only use as simple downstream \ac{NN}, the usage of a more complex classifier which leverages all output tokens could increase our models' performance as well.

\Textcite{Gao:2019} use \ac{BERT} to derive different models for aspect based sentiment analysis whose aim is to predict the polarity of a sentence's phrase.
They use the vanilla implementation of \ac{BERT} as baseline and use a dataset which consists of over $3,000$ restaurant as well as $3,000$ laptop reviews.
The baseline model again only uses the [CLS] token which is the input for a downstream \ac{NN}.
The more complex model uses the more output token of \ac{BERT}.
In addition to the [CLS] token, it also uses the output tokens of the sentence's target aspect.
Those token are then aggregated by an max-pooling layer before and then passed to a fully connected layer.
The authors obtained with vanilla \ac{BERT} a macro $F_1$ score of about $69\%$.
Their proposed model which uses more output token is able to boost the performance up to $79\%$ indicating that more complex downstream classifier can enhance the performance significantly.

Another transformer based approach is introduced by \textcite{Lee:2019}.
They use \ac{BERT} to classify patents and call it PatentBERT.
In their work the authors use the vanilla \ac{BERT} with hyperparameters recommended by \textcite{Devlin:2018} and only use the [CLS] output token for the downstream classification.
The model is trained with different portions of overall $3,050,615$ patents.
Although PatentBERT was trained on a massive dataset, this rather simple approach achieves $32.19\%$ precision, $86.06\%$ recall and a $F_1$ score of $46.85\%$.
This indicates that an extensive dataset is not sufficient to significantly enhance the performance of a model.

All previously presented approaches try to solve different classification problems using variants of \ac{BERT}.
Most authors use the vanilla version of \ac{BERT} as baseline and observe that this implementation performs rather poorly compared to altered and complexer versions.
Since the models we use correspond to the baseline models of the presented approaches which use a simple feed forward \ac{NN} for the downstream classification, we consider our results as plausible.
